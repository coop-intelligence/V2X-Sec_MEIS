<!doctype html>
<html>

<head>
  <title>V2X-Enhanced E2EAD - MEIS Challenge @ CVPR2025</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <script src="js/footer.js"></script>
  <style>
    .menu-index {
      color: rgb(255, 255, 255) !important;
      opacity: 1 !important;
      font-weight: 700 !important;
    }
  </style>
</head>

<body>
  <div class="menu-container"></div>
  <div class="content-container">
    <!-------------------------------------------------------------------------------------------->
    <!--Start Header-->
    <div class="banner" style="background: url('img/demo.gif') no-repeat center; background-size:contain; height: 250px;"></div>
    <div class="banner">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">End-to-End Autonomous Driving through V2X Cooperation <br>
               Challenge of MEIS Workshop @CVPR2025</h2>
            <p class="text">
              This challenge focuses on end-to-end autonomous driving with V2X cooperation, by utilizing 
              both ego-vehicle and infrastructure sensor data via V2X communication. This challenge can be 
              formulated as a planning-centric optimization with multiple-view sensor inputs under constrained 
              communication bandwidth.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--End Header-->
    <!-------------------------------------------------------------------------------------------->
    <div class="content">
      <div class="content-table flex-column">
        
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 id="Perception-track">Submission Instruction</h2>
            <p>
              Participants need to inference on test dataset and submit their result file to our portal.<br>
              Test Dataset has been released
              in <a href="https://drive.google.com/drive/folders/1zeMHE_k8mzxTxYHzXQpzDtpbzS9Z5dPL">this link</a> <br>
              
              As the test set was released 18 hours later than originally scheduled, the submission deadline has been extended by one day. <br>
              So the Final Submission Deadline is 23:59 (GMT+8), May 25, 2025.
            </p>
            <h3>Input and Output Requirements </h3>
              <h4>Input Constraints</h4>
                <li>Front Camera Images from Vehicle</li>
                <li>Camera Images from infrastructure</li>
                <li>Commands (turn left, go straight, or turn right)</li>
                <li>Ego Status at Current Frame </li>
                <li>Intrinsics and Extrinsics of Sensor</li>
              <h4>Output Constraints</h4>
                <li>Temporal perception results</li>
                <li>End-to-End planning results</li>
              <h4>Output Format Constraints</h4>
                <p>
                  The perception and planning results should be encapsulated in a <b>pkl-format document</b>. 
                  We provide an inference pipeline with official baseline <a href="https://github.com/AIR-THU/UniV2X/blob/main/docs/INFERENCE_TEST.md">here</a><br>
                  <b>Example:</b> We provide a sample inference file on the val dataset <a href="https://drive.google.com/file/d/1Rbwv12EzpWjyKb2vlXHOOnHWLq5eZQcY/view?usp=drive_link">here</a>, 
                  structured as follows:
                </p>
                <p>
                  if you want to submit results for Track 1 and Track 2:
                  <pre style="margin: 0%;padding: 0; white-space: pre-wrap;"><code>
{
    results: [
        {
            token: str
            # Tracking results
            boxes_3d: LiDARInstance3DBoxes
            scores_3d: Tensor, shape=(N,)
            labels_3d: Tensor, shape=(N,)
            track_scores: Tensor, shape=(N,)
            track_ids: Tensor, shape=(N,)
            # Detection results
            boxes_3d_det: LiDARInstance3DBoxes
            scores_3d_det: Tensor, shape=(M,)
            labels_3d_det: Tensor, shape=(M,)
            # Planning Results (future 5 seconds 2Hz traj points)
            planning_traj: Tensor, shape=(1, 10, 2)
        },
        {……},
        ……
    ]
}
                  </code></pre>
                  if you only want to submit results for Track 1:
                  <pre style="margin: 0%;padding: 0; white-space: pre-wrap;"><code>
{
    results: [
        {
            token: str
            # Tracking results
            boxes_3d: LiDARInstance3DBoxes
            scores_3d: Tensor, shape=(N,)
            labels_3d: Tensor, shape=(N,)
            track_scores: Tensor, shape=(N,)
            track_ids: Tensor, shape=(N,)
            # Detection results
            boxes_3d_det: LiDARInstance3DBoxes
            scores_3d_det: Tensor, shape=(M,)
            labels_3d_det: Tensor, shape=(M,)
        },
        {……},
        ……
    ]
}
                  </code></pre>
                  if you only want to submit results for Track 2:
                  <pre style="margin: 0%;padding: 0; white-space: pre-wrap;"><code>
{
    results: [
        {
            token: str
            # Planning Results (future 5 seconds 2Hz traj points)
            planning_traj: Tensor, shape=(1, 10, 2)
        },
        {……},
        ……
    ]
}
                  </code></pre>
                </p>
            <h3 style="margin-top: 0%;">Method Illustration Document</h3>
            <p class="text">
              Participants are encouraged to submit an <b>illustration file</b>, such as a published paper, an ArXiv paper, 
              a paper submitted to our workshop, or other technical reports, to explain their excellent methodology. <br>
              The file should provide sufficient details and analysis.
              A lack of explanation may undermine the credibility of the results. 
              Organizers may reach out to teams with insufficient documentation for further clarification.
            </p>
            <h3>Submission Portal</h3>
            <p class="text">
              Please submit your pkl-format result and method illustration to <a href="https://forms.gle/8M2HQ7LuUwyW8wMA6">this form</a>. <br>
              You may <b>edit and resubmit</b> this table as needed. The final version we receive will be considered the official submission. <br>
              Each team should designate <b>only one member</b> to submit the entry. <br>
              <b>Prohibition</b>: 1. Submitting results that are entirely based on existing others' open-sourced projects without any innovation is 
              strictly prohibited. 2. The use of additional training data is not allowed. 3. Historical ego status is not allowed in our challenge.
            </p>
      </div>
    </div>
    <div class="footer-container"></div>
  </div>
</body>

</html>
