<!doctype html>
<html>

<head>
  <title>V2X-Enhanced E2EAD - MEIS Challenge @ CVPR2025</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://use.fontawesome.com/releases/v5.2.0/css/all.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <script src="js/footer.js"></script>
  <style>
    .menu-index {
      color: rgb(255, 255, 255) !important;
      opacity: 1 !important;
      font-weight: 700 !important;
    }
  </style>
</head>

<body>
  <div class="menu-container"></div>
  <div class="content-container">
    <!-------------------------------------------------------------------------------------------->
    <!--Start Header-->
    <div class="banner" style="background: url('img/demo.gif') no-repeat center; background-size:contain; height: 250px;"></div>
    <div class="banner">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">End-to-End Autonomous Driving through V2X Cooperation <br>
               Challenge of MEIS Workshop @CVPR2025</h2>
            <p class="text">
              This challenge focuses on end-to-end autonomous driving with V2X cooperation, by utilizing 
              both ego-vehicle and infrastructure sensor data via V2X communication. This challenge can be 
              formulated as a planning-centric optimization with multiple-view sensor inputs under constrained 
              communication bandwidth.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--End Header-->
    <!-------------------------------------------------------------------------------------------->
    <div class="content">
      <div class="content-table flex-column">
        <!-------------------------------------------------------------------------------------------->
        <!--Start Intro-->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <p class="text add-top-margin">
              With the rapid advancement of autonomous driving technology, vehicle-to-everything (V2X) 
              communication has emerged as a key enabler for enhancing driving safety and efficiency. 
              By allowing ego-vehicles to exchange real-time information with surrounding infrastructure 
              and other road users, V2X communication extends perception capabilities beyond the line of 
              sight and mitigates the limitations of onboard sensors. However, integrating multi-source 
              sensor data from both ego-vehicles and infrastructure in a practical and efficient manner 
              remains a challenging task, especially under constrained communication bandwidth. </br>
              This challenge aims to tackle the problem of end-to-end autonomous driving with V2X cooperation 
              by leveraging both ego-vehicle and infrastructure sensor data. Specifically, we formulate the 
              problem as a planning-centric optimization, where multiple-view sensor inputs are fused to 
              generate robust driving planning results. By addressing the complexities of sensor fusion and 
              communication constraints, this challenge seeks to advance 
              the state-of-the-art in cooperative autonomous driving.
            </p>
            <h3>Challange Pre-Registration Link: <a href="https://forms.gle/tXWmT3EAhJqFKPGq5">here</a></h3>
            <h3>Table of Content</h3>
            <ul>
              <!-- <li><a href="#Open-loop-track">Open-loop Track</a></li> -->
              <li><a href="#Task_description">Task Description</a></li>
              <li><a href="#Timeline">Challenge Timeline</a></li>
              <li><a href="#Award">Award Setting</a></li>
              <li><a href="#Organizer_Committee">Organizer Committee</a></li>
              <li><a href="#Related-work">Related Work</a></li>
              <li><a href="#Contact">Contact</a></li>
            </ul>
          </div>
        </div>
        <!--End Intro-->
        <!-------------------------------------------------------------------------------------------->
        <!-- <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 id="Open-loop-track">Open-loop Track</h2>
            <hr>
            <h3> Task Overview </h3>
            <p class="text">
              The open-loop challenge focuses on evaluating the performance of V2X-enabled 
              autonomous driving in a non-interactive setting. Built upon the open-source 
              UniV2X framework, this challenge provides participants with pre-recorded 
              multi-view sensor data from both ego-vehicles and infrastructure. The primary 
              objective is to leverage V2X communication for improved perception and 
              planning under constrained bandwidth conditions. Participants are 
              required to process the provided data to generate high-quality driving plans 
              without real-time environmental feedback.
            </p>
            <h3>Evaluation</h3>
            <p class="text">
              Participants' solutions will be assessed based on two common metrics: L2 and Collision Rate.
            </p>
            <h3>Baseline and Data Illustration</h3>
            <p class="text">
              UniV2X (AAAI 2025) is the first cooperative autonomous driving framework that seamlessly integrates 
              all key driving modules across diverse driving views into a unified network. The code is 
              open-sourced in <a href="https://github.com/AIR-THU/UniV2X">this page</a>. <br>
              The dataset is based on V2X-Seq-SPD, you may see more details 
              <a href="https://github.com/AIR-THU/UniV2X/blob/main/docs/DATA_PREP.md">here</a>.
            </p>
          </div>
        </div> -->
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 id="Task_description">Task Description</h2>
            <hr>
            <h3> Task Overview </h3>
            <p class="text">
              The challenge is evaluated in a closed-loop setting. Built on the upcoming 
              DriveV2X platform, this challenge enables participants to develop and test 
              autonomous driving agents that actively engage with dynamic traffic conditions 
              using V2X cooperation. The goal is to optimize end-to-end driving policies that 
              adaptively fuse ego-vehicle and infrastructure sensor data while navigating 
              complex urban scenarios.
            </p>
            <h3>Evaluation</h3>
            <p class="text">
              Participants' solutions will be assessed based on two key common metrics: Success Rate and Driving Score. <br>
              We also encourage participants submitted open-loop metrics including L2 and collision rate as supplementary.
            </p>
            <h3>Baseline and Codebase</h3>
            <p class="text">
              The complex driving scenarios are generated using a novel digital-twin-based simulation framework (DriveE2E), whose rendering is based on Carla. 
              In this setup, we reimplemented UniV2X as the baseline model, ultimately forming the <b>DriveV2X</b> framework.
              UniV2X (AAAI 2025) is the first cooperative autonomous driving framework that seamlessly integrates all 
              key driving modules across diverse driving perspectives into a unified network. <br> <br>
              DriveE2E framework is avalible at: <a href="https://github.com/AIR-THU/DriveE2E">https://github.com/AIR-THU/DriveE2E</a>. <br>
              UniV2X is avalible at: <a href="https://github.com/AIR-THU/UniV2X">https://github.com/AIR-THU/UniV2X</a>. <br>
              DriveV2X is comming soon.
            </p>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 id="Timeline">Challenge Timeline</h2>
            <hr>
            <ul>
              <li>Release of the comprehensive DriveV2X: March 15, 2025</li>
              <li>Submission open: April 10, 2025</li>
              <li>Submission deadline: May 28, 2025</li>
              <li>Decision to authors: June 4, 2025</li>
            </ul>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 id="Award">Award Setting</h2>
            <hr>
            <ul>
              <li><img src="./img/85556_gold_trophy_icon.png" alt="Figure of gold" width="20" height="20"></img>Outstanding Champion, USD $1500</li>
              <li><img src="./img/85557_silver_trophy_icon.png" alt="Figure of silver" width="20" height="20">Honorable Runner-up, USD $1000</li>
              <li><img src="./img/85555_bronze_trophy_icon.png" alt="Figure of bronze" width="20" height="20">Exceptional Merit Award, USD $500</li>
            </ul>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 id="Organizer_Committee">Organizer Committee</h2>
            <hr>
            <p class="text"> Ruiyang Hao (THU), Haibao Yu (HKU), Chuanye Wang (BHU), Zihao Lu (ETH), Jiayu Wu (BIT), <br>
              Jiaru Zhong (BIT), Jiahao Wang (THU), Wenxian Yang (THU), Siqi Fan (THU)
            </p>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 id="Related-work">Related Work</h2>
            <hr>
            <ul>
              <li><a href="https://arxiv.org/abs/2404.00717">UniV2X</a>, AAAI 2025</li>
              <li><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_DAIR-V2X_A_Large-Scale_Dataset_for_Vehicle-Infrastructure_Cooperative_3D_Object_Detection_CVPR_2022_paper.pdf">DAIR-V2X</a>, CVPR 2022</li>
            </ul>
          </div>
        </div>
        <!-------------------------------------------------------------------------------------------->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 id="Contact">Contact</h2>
            <hr>
            <ul>
              <li>Challenge Process: Ruiyang Hao, THU, email: hry20@tsinghua.org.cn</li>
              <li>Open-loop Track Support: Jiaru Zhong, BIT, email: zhongjiaru@bit.edu.cn</li>
              <li>Closed-loop Track Support: Haibao Yu, HKU, email: yuhaibao94@gmail.com</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
    <div class="footer-container"></div>
  </div>
</body>

</html>
